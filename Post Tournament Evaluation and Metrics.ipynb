{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Tournament Evaluation and Metrics\n",
    "\n",
    "In this notebook, we'll take a look at how our bracket did compared to the actual tournament results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import collegebasketball as cbb\n",
    "cbb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Scores from Last Season\n",
    "Now that the season is complete, we can retrieve all of the scores for both evaluating our bracket this year and as more training data for next year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to search for games\n",
    "year = 2022\n",
    "start = datetime.date(year - 1, 11, 1)\n",
    "end = datetime.date(year, 4, 10)\n",
    "\n",
    "# Set up the path for this years scores\n",
    "path = './Data/Scores/'\n",
    "path_regular = path + str(year) + '_season.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbb.load_scores_dataframe(start, end, csv_file_path=path_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_regular)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Predictions and Kenpom Data for This Season\n",
    "In addition to the actual tournament game scores, we'll need our predictions and the pre-tournament Kenpom data to evaluate our bracket. The scores are obviously needed to verify when we were correct, but the Kenpom data is also necessary to determine which team was favored in each game since our model determines favorites using the Kenpom efficiency metric rather than tournament seeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cbb.filter_tournament(data)\n",
    "predictions = pd.read_csv(f'./Data/predictions/predictions_{year}.csv')\n",
    "kenpom = pd.read_csv(f'./Data/Kenpom/{year}_kenpom.csv')\n",
    "kenpom = cbb.update_kenpom(kenpom)\n",
    "kenpom.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics\n",
    "\n",
    "Now that we have all the necessary data, we can use a function to get all the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation function on our data\n",
    "cbb.post_tournament_eval(predictions, scores, kenpom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on the various metrics above:\n",
    "* **Games Contained Predicted Winner**: The number of games my bracket's predicted winner actually played in. In later rounds, my predicted winner may have already lost in a previous round so my predicted winner may not have even played in the game.\n",
    "* **Total Upsets:** The number of actual upsets where an upset is defined as a team with a lower Kenpom efficiency score winning the game. Note this is referring the actual tournament results.\n",
    "* **Upsets Predicted:** The number of games I predicted an upset based on the two teams I predicted to be playing in my bracket.\n",
    "* **Games Containing Actual Upset Winner:** The number of games that were actual upsets where my bracket had the winning team predicted to be in the game.\n",
    "* **Games Containing Predicted Upset Winner:** The number of games I predicted an upset where my predicted upset winner actually played. \n",
    "* **Correct Upsets:** The number of games where the actual winner was an underdog that I correctly predicted would win.\n",
    "* **Correct Predicted Upsets:** The number of games where I predicted an upset and that team actually won the game.\n",
    "* **Total Accuracy:** The fraction of all games where I correctly predicted the winner.\n",
    "* **Upset Precision:** The fraction of all upsets I predicted that were correct.\n",
    "* **Upset Recall:** The fraction of all actual upsets I predicted correctly.\n",
    "* **Adj Accuracy:** The fraction of all games containing my predicted winner that I predicted correctly.\n",
    "* **Adj Upset Precision:** The fraction of all games containing my predicted upset winner that I predicted correctly.\n",
    "* **Adj Upset Recall:** The fraction of all actual upsets containing my predicted winner that I predicted correctly. \n",
    "\n",
    "The purpose of these \"adjusted\" stats are to adjust for the fact that in later rounds, it might not have even been possible to make a correct prediction based on previous errors. While I can see how you would want to know the full accuracy numbers for every game, I think these \"adjusted\" metrics provide a more accurate measure of performance on a game by game basis so that is why I've calculated them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did the Bracket Perform this Year?\n",
    "\n",
    "To start off with, I'm excited to say that the bracket successfully predicted the winner for the third time out of four years. It's honestly incredible how frequently this bracket has predicted the winner even if you account for some favorites winning it over the last few years and I honestly don't expect this trend to continue.\n",
    "\n",
    "Overall, it was a pretty average year as far as the metrics go. The overall accuracy and adjusted accuracy were down compared to previous seasons, but the upset precision and recall metrics were overall slightly up. This is mostly due to there being more upsets than previous seasons, so I think it's fair to say that the model actually was better than previous years, but the frequent number of upsets caused overall accuracy to be lower in spite of that.\n",
    "\n",
    "Unfortunately, I haven't found much time to make many improvements for next year, but I'm looking forward to seeing how the bracket performs next tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,markdown//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
